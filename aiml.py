# -*- coding: utf-8 -*-
"""AIML.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/12HkhOMZkGK5I06Dy91tCzJKSjDAc8pnI
"""

import pandas as pd
df = pd.read_excel('star_classification.csv.xlsx')

print(df.shape)
print(df.info())
print(df.head())

print(df.describe())

print(df.isnull().sum())

print(df['class'].value_counts())

import pandas as pd
import numpy as np # Needed for np.nan

# Define the file name for the star classification data
file_name = 'star_classification.csv.xlsx'

# Load the dataset into a pandas DataFrame
try:
    # Use pd.read_excel for .xlsx files
    df = pd.read_excel(file_name)
    print(f"Successfully loaded '{file_name}'")
except FileNotFoundError:
    print(f"Error: '{file_name}' not found. Please ensure the file is in the correct directory.")
    # Exit or handle the error appropriately if the file is essential
    exit()
# Add an except block for potential issues when reading the Excel file
except Exception as e:
    print(f"An error occurred while reading the Excel file: {e}")
    exit()


# Define the spectral features that have the -9999.0 placeholder
spectral_features_to_clean = ['u', 'g', 'z']

print(f"\nOriginal DataFrame shape: {df.shape}")

# Replace -9999.0 with NaN (Not a Number) in the specified columns
for col in spectral_features_to_clean:
    # Ensure the column exists before attempting to replace values
    if col in df.columns:
        df[col] = df[col].replace(-9999.0, np.nan)
        print(f"Replaced -9999.0 with NaN in column: '{col}'")
    else:
        print(f"Warning: Column '{col}' not found in the DataFrame.")


# Drop rows where any of the specified spectral features (u, g, z) now contain NaN.
# This effectively removes rows that had the -9999.0 placeholder.
df_cleaned = df.dropna(subset=spectral_features_to_clean)

print(f"\nCleaned DataFrame shape: {df_cleaned.shape}")

# You can optionally display the first few rows of the cleaned DataFrame
print("\nFirst 5 rows of the cleaned DataFrame:")
print(df_cleaned.head())

# And check for missing values again in the cleaned columns to confirm
print("\nMissing values in cleaned spectral features (should be 0):")
print(df_cleaned[spectral_features_to_clean].isnull().sum())

# Generate descriptive statistics for the cleaned DataFrame
print("\nDescriptive statistics:")
print(df_cleaned.describe())

import matplotlib.pyplot as plt

spectral_features = ['u', 'g', 'r', 'i', 'z']

df[spectral_features].hist(bins=30, figsize=(15,10))
plt.suptitle('Histograms of Spectral Features (u, g, r, i, z)')
plt.show()

subset_cols = ['u', 'g', 'r', 'i', 'z', 'class']

sns.pairplot(df[subset_cols], hue='class', diag_kind='kde', plot_kws={'alpha':0.5})
plt.suptitle('Pairplot of Spectral Features Colored by Class', y=1.02)
plt.show()

rows_removed = df.shape[0] - df_cleaned.shape[0]
print(f"\nTotal rows removed: {rows_removed}")

corr = df[['u', 'g', 'r', 'i', 'z']].corr()

plt.figure(figsize=(8,6))
sns.heatmap(corr, annot=True, fmt=".2f", cmap='coolwarm')
plt.title('Correlation Matrix of Spectral Features')
plt.show()

from sklearn.linear_model import LinearRegression
from sklearn.metrics import r2_score
import matplotlib.pyplot as plt
import seaborn as sns

# Define the threshold for high correlation
threshold = 0.85

# Step 1: Identify highly correlated pairs (using the original correlation matrix calculated on the full df)
# While the correlation matrix was calculated on df (which has NaNs), the pairs identified
# are just the column names. The issue is in the data used for the regression.
high_corr_pairs = []
for i in range(len(corr.columns)):
    for j in range(i + 1, len(corr.columns)):
        if abs(corr.iloc[i, j]) >= threshold:
            feature_x = corr.columns[i]
            feature_y = corr.columns[j]
            # Only include pairs where both features are present in the columns we cleaned NaNs from,
            # or if they weren't part of the cleaned columns, they should not have had -9999 anyway.
            # We will use df_cleaned for the regression, which handles rows that originally had -9999.
            high_corr_pairs.append((feature_x, feature_y, corr.iloc[i, j]))

# Step 2: Linear regression for each high correlation pair
for x_feat, y_feat, corr_value in high_corr_pairs:
    print(f"\n🔍 Linear Regression between: {x_feat} and {y_feat} (corr = {corr_value:.2f})")

    # Prepare data - USE THE CLEANED DATAFRAME
    # Ensure the features exist in the cleaned dataframe
    if x_feat in df_cleaned.columns and y_feat in df_cleaned.columns:
        X = df_cleaned[[x_feat]]
        y = df_cleaned[y_feat]

        # Fit model
        model = LinearRegression()
        # The fit method will now receive data without NaNs (for the rows kept in df_cleaned)
        model.fit(X, y)
        y_pred = model.predict(X)

        # Coefficients
        slope = model.coef_[0]
        intercept = model.intercept_
        r2 = r2_score(y, y_pred)

        print(f"Equation: {y_feat} = {slope:.4f} * {x_feat} + {intercept:.4f}")
        print(f"R² Score: {r2:.4f}")


        plt.figure(figsize=(8, 6))
        sns.scatterplot(x=x_feat, y=y_feat, data=df_cleaned, alpha=0.4, label='Actual Data')
        # Plot the regression line using the X values from the cleaned data
        plt.plot(df_cleaned[x_feat], y_pred, color='red', label='Regression Line')
        plt.title(f'Linear Regression: {y_feat} vs {x_feat}')
        plt.xlabel(x_feat)
        plt.ylabel(y_feat)
        plt.legend()
        plt.grid(True)
        plt.tight_layout()
        plt.show()
    else:
        print(f"Warning: Features '{x_feat}' or '{y_feat}' not found in the cleaned DataFrame.")

print("\nHighly correlated pairs (above threshold):")
for i in range(len(corr.columns)):
    for j in range(i + 1, len(corr.columns)):
        val = corr.iloc[i, j]
        if abs(val) >= threshold:
            print(f"{corr.columns[i]} & {corr.columns[j]}: {val:.2f}")

import missingno as msno
msno.matrix(df)
plt.show()

plt.figure(figsize=(12,6))
sns.boxplot(data=df[spectral_features])
plt.title('Boxplot of Spectral Features')
plt.show()

for col in spectral_features:
    plt.figure(figsize=(8, 4))
    sns.boxplot(data=df_cleaned, x='class', y=col)
    plt.title(f'Boxplot of {col}-band by Class')
    plt.show()

for feature in spectral_features:
    plt.figure(figsize=(10,5))
    sns.kdeplot(data=df, x=feature, hue='class', common_norm=False)
    plt.title(f'Distribution of {feature} by Class')
    plt.show()

df['u_g'] = df['u'] - df['g']
df['g_r'] = df['g'] - df['r']
df['r_i'] = df['r'] - df['i']
df['i_z'] = df['i'] - df['z']

print(df[['u_g', 'g_r', 'r_i', 'i_z']].head())
sns.pairplot(df[['u_g', 'g_r', 'r_i', 'i_z', 'class']], hue='class')

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# 1. Select features and target
X = df[['u_g', 'g_r', 'r_i', 'i_z']].dropna()
y = df.loc[X.index, 'class']  # Align target with filtered features

# 2. Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Train SVM with RBF kernel (default)
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
svm_rbf.fit(X_train, y_train)

# 4. Predict on train and test sets
y_train_pred = svm_rbf.predict(X_train)
y_test_pred = svm_rbf.predict(X_test)

# 5. Accuracy
train_acc = accuracy_score(y_train, y_train_pred)
test_acc = accuracy_score(y_test, y_test_pred)

print(f"Training Accuracy (SVM-RBF): {train_acc:.2f}")
print(f"Test Accuracy (SVM-RBF): {test_acc:.2f}")

# 6. Predict and show output for a new or specific test sample
# Example: Predict the first test sample
sample = X_test.iloc[[0]]  # Double brackets to keep it as a DataFrame
predicted_class = svm_rbf.predict(sample)[0]
print(f"Predicted Class: {predicted_class}")

X.hist(bins=30, figsize=(10, 8))
plt.show()

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Assuming df is your input DataFrame with columns 'u', 'g', 'r', 'i', 'z', 'class'
# Replace this with your actual data loading if needed
# df = pd.read_csv('your_data.csv')  # Uncomment and adjust as needed

# Step 1: Remove rows with placeholder values (-9999)
df_clean = df[(df['u'] > -1000) & (df['g'] > -1000) &
              (df['r'] > -1000) & (df['i'] > -1000) &
              (df['z'] > -1000)]

# Step 2: Calculate color features
df_clean['u_g'] = df_clean['u'] - df_clean['g']
df_clean['g_r'] = df_clean['g'] - df_clean['r']
df_clean['r_i'] = df_clean['r'] - df_clean['i']
df_clean['i_z'] = df_clean['i'] - df_clean['z']

# Step 3: Prepare features and target, drop NaNs
X = df_clean[['u_g', 'g_r', 'r_i', 'i_z']].dropna()
y = df_clean.loc[X.index, 'class']

# Step 4: Check dataset size and class distribution
print("Dataset size:", X.shape)
print("Class distribution:\n", y.value_counts())

# Step 5: Scale features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Step 6: Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X_scaled, y, test_size=0.3, random_state=42
)

# Step 7: Unsupervised Learning - KMeans
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)  # Assuming 3 classes
clusters = kmeans.fit_predict(X_scaled)

# Visualize clusters (using u_g and g_r)
plt.figure(figsize=(8, 6))
plt.scatter(X['u_g'], X['g_r'], c=clusters, cmap='viridis', s=50)
plt.title('KMeans Clustering (Unsupervised)')
plt.xlabel('u_g')
plt.ylabel('g_r')
plt.colorbar(label='Cluster')
plt.show()

# Step 8: Hyperparameter tuning for SVM
param_grid = {
    'C': [0.1, 1, 10, 100],
    'gamma': ['scale', 'auto', 0.001, 0.01, 0.1, 1]
}
grid = GridSearchCV(SVC(kernel='rbf', random_state=42), param_grid, cv=5, n_jobs=-1)
grid.fit(X_train, y_train)

print("Best parameters:", grid.best_params_)
print("Best cross-validation score:", grid.best_score_)

# Step 9: Train SVM with best parameters
svm_rbf = SVC(kernel='rbf', **grid.best_params_, random_state=42)
svm_rbf.fit(X_train, y_train)

# Step 10: Predict
y_train_pred = svm_rbf.predict(X_train)
y_test_pred = svm_rbf.predict(X_test)

# Step 11: Calculate accuracy
train_acc = accuracy_score(y_train, y_train_pred)
test_acc = accuracy_score(y_test, y_test_pred)
print(f"Training Accuracy (SVM-RBF): {train_acc:.2f}")
print(f"Test Accuracy (SVM-RBF): {test_acc:.2f}")

# Step 12: Predict class of a sample from test set
sample = X_test[[0]]  # Use index-based selection for NumPy array
predicted_class = svm_rbf.predict(sample)[0]
print(f"Predicted Class for Sample: {predicted_class}")

# Step 13: Model fit evaluation
if train_acc > test_acc + 0.1:
    model_fit = "Overfit"
elif abs(train_acc - test_acc) < 0.05 and train_acc > 0.9:
    model_fit = "Best Fit"
else:
    model_fit = "Underfit"
print(f"Model Fit: {model_fit}")

# file ipython-input-31-262cb0067422
from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score
from sklearn.cluster import KMeans
import matplotlib.pyplot as plt
import pandas as pd
import numpy as np

# Step 1: Remove rows with placeholder values (-9999)
df_clean = df[(df['u'] > -1000) & (df['g'] > -1000) &
              (df['r'] > -1000) & (df['i'] > -1000) &
              (df['z'] > -1000)]

# Step 2: Calculate color features safely
df_clean['u_g'] = df_clean['u'] - df_clean['g']
df_clean['g_r'] = df_clean['g'] - df_clean['r']
df_clean['r_i'] = df_clean['r'] - df_clean['i']
df_clean['i_z'] = df_clean['i'] - df_clean['z']

# Step 3: Drop any NaNs (just in case)
X = df_clean[['u_g', 'g_r', 'r_i', 'i_z']].dropna()
y = df_clean.loc[X.index, 'class']


# ---------------------
# Unsupervised Learning: KMeans
# ---------------------
# Ensure n_init is specified to suppress future warnings in scikit-learn v1.2+
kmeans = KMeans(n_clusters=3, random_state=42, n_init=10)  # Assuming 3 classes: Star, Galaxy, QSO
clusters = kmeans.fit_predict(X)

# Optional: Visualize clusters (using 2D projection)
plt.figure(figsize=(8, 6))
# Use the filtered X DataFrame for plotting as well
plt.scatter(X['u_g'], X['g_r'], c=clusters, cmap='viridis', s=50)
plt.title('KMeans Clustering (unsupervised)')
plt.xlabel('u_g')
plt.ylabel('g_r')
plt.colorbar(label='Cluster')
plt.show()

# ---------------------
# Supervised Learning: SVM
# ---------------------
# 2. Split into train and test sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# 3. Train SVM with RBF kernel
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
svm_rbf.fit(X_train, y_train)

# 4. Predict
y_train_pred = svm_rbf.predict(X_train)
y_test_pred = svm_rbf.predict(X_test)

# 5. Accuracy
train_acc = accuracy_score(y_train, y_train_pred)
test_acc = accuracy_score(y_test, y_test_pred)

print(f"Training Accuracy (SVM-RBF): {train_acc:.2f}")
print(f"Test Accuracy (SVM-RBF): {test_acc:.2f}")

# 6. Predict class of a sample from test set
sample = X_test.iloc[[0]]
predicted_class = svm_rbf.predict(sample)[0]
print(f"Predicted Class: {predicted_class}")
#bestfit or underfit
if train_accuracy > test_accuracy + 0.1:
    model_fit = "Overfit"
elif abs(train_accuracy - test_accuracy) < 0.05 and train_accuracy > 0.9:
    model_fit = "Best Fit"
else:
    model_fit = "Underfit"

print(f"Model Fit: {model_fit}")

from sklearn.model_selection import train_test_split
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

# Split the data
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.3, random_state=42
)

# Train SVM with RBF kernel
svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)
svm_rbf.fit(X_train, y_train)

# Predict
y_train_pred = svm_rbf.predict(X_train)
y_test_pred = svm_rbf.predict(X_test)

# Accuracy
train_acc = accuracy_score(y_train, y_train_pred)
test_acc = accuracy_score(y_test, y_test_pred)

# Model fit evaluation
if train_acc > test_acc + 0.1:
    model_fit = "Overfit"
elif abs(train_acc - test_acc) < 0.05 and train_acc > 0.9:
    model_fit = "Best Fit"
else:
    model_fit = "Underfit"

print(f"Model Fit: {model_fit}")

from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay
import matplotlib.pyplot as plt

# 1. Compute the confusion matrix
cm = confusion_matrix(y_test, y_test_pred, labels=svm_rbf.classes_)

# 2. Display the confusion matrix
disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=svm_rbf.classes_)
plt.figure(figsize=(8, 6))
disp.plot(cmap='Blues', values_format='d')
plt.title("Confusion Matrix - SVM with RBF Kernel")
plt.show()